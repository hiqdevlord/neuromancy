DONE:
+ sigmoid, sigmoid gradient
+ randomize weights (initialize network)
+ feed forward (predict)
+ calculate deltas (errors)
+ calculate gradients
+ update weights
+ calculate cost
+ learn weights (loop over the preceding 5 functions)


TODO:
- create a mini-batch learning function
- set up some unit tests (i.e., move tests from __main__ into their own files)
- train with fmincg instead of gradient descent

- focus on good ML practices:
	- implement cross validation
	- plot learning curves
	- run error diagnostics
	- optimize learning rate and regularization coefficient
	- anomaly detection?

- fun stuff:
	- display weights as a heat map
	- display images from test set
